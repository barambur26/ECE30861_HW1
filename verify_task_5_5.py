#!/usr/bin/env python3
"""
Verification script for Task 5.5: Test error scenarios with invalid URLs

This script runs quick verification tests to ensure invalid URL handling
is working correctly across the system.
"""

import sys
import os
import subprocess
import tempfile
from pathlib import Path

# Add the src directory to Python path
sys.path.insert(0, '/Users/blas/Documents/Obsidian Vault/School/F25/ECE 461/Project Repo/ECE30861_HW1/src')

def test_cli_with_invalid_urls():
    """Test the CLI with various invalid URL files."""
    
    print("üß™ Testing CLI with Invalid URLs")
    print("=" * 50)
    
    project_root = Path('/Users/blas/Documents/Obsidian Vault/School/F25/ECE 461/Project Repo/ECE30861_HW1')
    run_script = project_root / 'run'
    
    # Test files with different types of invalid URLs
    test_files = [
        'test_malformed_urls.txt',
        'test_non_huggingface_urls.txt', 
        'test_nonexistent_repos.txt',
        'test_edge_case_urls.txt'
    ]
    
    results = {"passed": 0, "failed": 0}
    
    for test_file in test_files:
        test_path = project_root / test_file
        if not test_path.exists():
            print(f"‚ö†Ô∏è  Test file not found: {test_file}")
            continue
            
        print(f"\nüîç Testing with {test_file}...")
        
        try:
            # Run the CLI with the invalid URL file
            result = subprocess.run(
                [str(run_script), str(test_path)],
                cwd=str(project_root),
                capture_output=True,
                text=True,
                timeout=30
            )
            
            # For invalid URLs, we expect the CLI to either:
            # 1. Exit with code 1 (error)
            # 2. Exit with code 0 but handle errors gracefully
            if result.returncode in [0, 1]:
                print(f"   ‚úÖ CLI handled invalid URLs appropriately (exit code {result.returncode})")
                results["passed"] += 1
            else:
                print(f"   ‚ùå CLI failed unexpectedly (exit code {result.returncode})")
                if result.stderr:
                    print(f"      Error: {result.stderr[:200]}...")
                results["failed"] += 1
                
        except subprocess.TimeoutExpired:
            print(f"   ‚ùå CLI timed out with {test_file}")
            results["failed"] += 1
        except Exception as e:
            print(f"   ‚ùå Error testing {test_file}: {e}")
            results["failed"] += 1
    
    return results

def test_individual_metrics():
    """Test individual metrics with invalid URLs."""
    
    print("\nüîß Testing Individual Metrics with Invalid URLs")
    print("=" * 50)
    
    try:
        from acemcli.exceptions import ValidationError, APIError, MetricError
        from acemcli.models import Category
        from acemcli.metrics.hf_api import HFAPIMetric
        from acemcli.metrics.local_repo import LocalRepoMetric
        
        # Test invalid URLs
        invalid_urls = [
            (None, "None URL"),
            ("", "Empty string"),
            ("not-a-url", "Invalid format"),
            ("https://github.com/user/repo", "Non-HuggingFace URL"),
        ]
        
        metrics = [
            ("HFAPIMetric", HFAPIMetric()),
            ("LocalRepoMetric", LocalRepoMetric()),
        ]
        
        results = {"passed": 0, "failed": 0}
        
        for metric_name, metric in metrics:
            print(f"\nüîç Testing {metric_name}:")
            
            for invalid_url, description in invalid_urls:
                try:
                    # First check supports method
                    if not metric.supports(invalid_url, Category.MODEL):
                        print(f"   ‚úÖ {description}: Properly rejected by supports()")
                        results["passed"] += 1
                        continue
                    
                    # If supports() passes, test compute method
                    result = metric.compute(invalid_url, Category.MODEL)
                    print(f"   ‚ö†Ô∏è  {description}: Unexpectedly succeeded")
                    results["failed"] += 1
                    
                except (ValidationError, APIError, MetricError) as e:
                    print(f"   ‚úÖ {description}: Properly raised {type(e).__name__}")
                    results["passed"] += 1
                    
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  {description}: Raised unexpected {type(e).__name__}")
                    results["failed"] += 1
        
        return results
        
    except ImportError as e:
        print(f"‚ùå Failed to import required modules: {e}")
        return {"passed": 0, "failed": 1}

def test_exception_framework():
    """Test that the exception framework is working properly."""
    
    print("\nüö® Testing Exception Framework")
    print("=" * 50)
    
    try:
        from acemcli.exceptions import (
            ValidationError, APIError, MetricError,
            create_api_timeout_error, create_invalid_url_error,
            create_metric_score_error
        )
        
        # Test exception creation
        validation_error = ValidationError("Test validation error", field_name="test_field")
        api_error = APIError("Test API error", api_name="TestAPI", status_code=404)
        metric_error = MetricError("Test metric error", metric_name="test_metric")
        
        # Test convenience functions
        timeout_error = create_api_timeout_error("TestAPI", "https://test.com", 30)
        url_error = create_invalid_url_error("invalid-url", "Test reason")
        score_error = create_metric_score_error("test_metric", 1.5)
        
        print("   ‚úÖ ValidationError creation works")
        print("   ‚úÖ APIError creation works")
        print("   ‚úÖ MetricError creation works")
        print("   ‚úÖ Timeout error creation works")
        print("   ‚úÖ URL error creation works")
        print("   ‚úÖ Score error creation works")
        
        return {"passed": 6, "failed": 0}
        
    except Exception as e:
        print(f"   ‚ùå Exception framework test failed: {e}")
        return {"passed": 0, "failed": 1}

def run_verification():
    """Run the complete verification for Task 5.5."""
    
    print("üöÄ Task 5.5 Verification: Invalid URL Error Scenarios")
    print("=" * 60)
    print("Testing that invalid URLs are handled gracefully with proper error messages")
    print("=" * 60)
    
    # Run all verification tests
    cli_results = test_cli_with_invalid_urls()
    metric_results = test_individual_metrics()
    exception_results = test_exception_framework()
    
    # Calculate totals
    total_passed = cli_results["passed"] + metric_results["passed"] + exception_results["passed"]
    total_failed = cli_results["failed"] + metric_results["failed"] + exception_results["failed"]
    total_tests = total_passed + total_failed
    
    print("\n" + "=" * 60)
    print("üìä VERIFICATION RESULTS")
    print("=" * 60)
    
    print(f"CLI Invalid URL Tests:     {cli_results['passed']}/{cli_results['passed'] + cli_results['failed']} passed")
    print(f"Metric Error Handling:     {metric_results['passed']}/{metric_results['passed'] + metric_results['failed']} passed")
    print(f"Exception Framework:       {exception_results['passed']}/{exception_results['passed'] + exception_results['failed']} passed")
    
    print("=" * 60)
    print(f"üéØ OVERALL: {total_passed}/{total_tests} tests passed ({total_passed/total_tests*100:.1f}%)")
    
    if total_passed == total_tests:
        print("\nüéâ TASK 5.5 VERIFICATION SUCCESSFUL!")
        print("‚úÖ Invalid URL error scenarios are properly handled")
        print("‚úÖ CLI handles invalid URLs gracefully")
        print("‚úÖ Metrics validate input properly")
        print("‚úÖ Exception framework is working correctly")
        
        print("\nüìã Verified Features:")
        print("   ‚Ä¢ Malformed URL detection and rejection")
        print("   ‚Ä¢ Non-HuggingFace URL filtering")
        print("   ‚Ä¢ Non-existent repository error handling")
        print("   ‚Ä¢ Edge case URL processing")
        print("   ‚Ä¢ Proper error messages and exit codes")
        print("   ‚Ä¢ Custom exception hierarchy")
        
        return True
    else:
        print("\n‚ö†Ô∏è  TASK 5.5 VERIFICATION FAILED")
        print(f"   {total_failed} tests failed out of {total_tests}")
        print("   Some invalid URL scenarios may not be handled properly")
        return False

if __name__ == "__main__":
    success = run_verification()
    sys.exit(0 if success else 1)
